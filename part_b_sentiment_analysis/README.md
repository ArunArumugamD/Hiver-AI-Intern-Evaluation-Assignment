# Part B: Sentiment Analysis Prompt Evaluation

## Overview

Evaluation of sentiment analysis prompts for customer support emails with iterative improvements.

## Prompt v1: Initial Approach

### Design

[To be filled: Describe your initial prompt design]

### Results on 10 Test Emails

| Email ID | True Sentiment | Predicted | Confidence | Correct? |
|----------|---------------|-----------|------------|----------|
| 1        | [TBD]         | [TBD]     | [TBD]      | [TBD]    |
| 2        | [TBD]         | [TBD]     | [TBD]      | [TBD]    |
| ...      | ...           | ...       | ...        | ...      |

**Accuracy**: [X]%

## What Failed (v1)

### Failure Pattern 1: [Title]

**Issue**: [Description]

**Examples**:
- [Example 1]
- [Example 2]

**Root Cause**: [Analysis]

### Failure Pattern 2: [Title]

**Issue**: [Description]

**Examples**:
- [Example 1]
- [Example 2]

**Root Cause**: [Analysis]

### Failure Pattern 3: [Title]

**Issue**: [Description]

**Examples**:
- [Example 1]
- [Example 2]

**Root Cause**: [Analysis]

## What Was Improved (v2)

### Improvement 1: [Title]

**Problem Addressed**: [From v1 failures]

**Solution**: [What changed in prompt]

**Impact**: [How it improved results]

### Improvement 2: [Title]

**Problem Addressed**: [From v1 failures]

**Solution**: [What changed in prompt]

**Impact**: [How it improved results]

### Improvement 3: [Title]

**Problem Addressed**: [From v1 failures]

**Solution**: [What changed in prompt]

**Impact**: [How it improved results]

## Prompt v2: Improved Approach

### Design Changes

[To be filled: Describe improvements made]

### Results on 10 Test Emails

| Email ID | True Sentiment | Predicted | Confidence | Correct? |
|----------|---------------|-----------|------------|----------|
| 1        | [TBD]         | [TBD]     | [TBD]      | [TBD]    |
| 2        | [TBD]         | [TBD]     | [TBD]      | [TBD]    |
| ...      | ...           | ...       | ...        | ...      |

**Accuracy**: [X]%

**Improvement**: +[X]% from v1

## How to Evaluate Prompts Systematically

### 1. Test Set Design

[To be filled: How you selected/created test cases]

### 2. Evaluation Metrics

**Quantitative**:
- Accuracy
- Confidence calibration
- Inter-annotator agreement (if applicable)

**Qualitative**:
- Reasoning quality
- Edge case handling
- Consistency

### 3. Iteration Process

```
1. Design initial prompt → Test → Analyze failures
2. Identify patterns in errors
3. Hypothesize improvements
4. Modify prompt → Re-test
5. Compare results → Document learnings
```

### 4. Best Practices Discovered

1. **[Best Practice 1]**: [Description and rationale]
2. **[Best Practice 2]**: [Description and rationale]
3. **[Best Practice 3]**: [Description and rationale]

## Key Learnings

### What Works

- [Learning 1]
- [Learning 2]
- [Learning 3]

### What Doesn't Work

- [Anti-pattern 1]
- [Anti-pattern 2]
- [Anti-pattern 3]

### Surprising Findings

[To be filled: Any unexpected results or insights]

## Recommendations for Production

1. **[Recommendation 1]**: [Description]
2. **[Recommendation 2]**: [Description]
3. **[Recommendation 3]**: [Description]

## Conclusion

[To be filled: Summary of the evaluation process and key takeaways]
