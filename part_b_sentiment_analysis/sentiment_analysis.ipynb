{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Sentiment Analysis Prompt Evaluation\n",
    "\n",
    "## Objective\n",
    "- Create sentiment analysis prompt (positive/negative/neutral)\n",
    "- Include confidence score and reasoning\n",
    "- Test on 10 emails\n",
    "- Iterate and improve (v1 → v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data (10 Emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 10 emails\n",
      "\n",
      "Sample emails:\n",
      "\n",
      "Email 1:\n",
      "Subject: Unable to access shared mailbox\n",
      "Body: Hi team, I'm unable to access the shared mailbox for our support team. It keeps showing a permission...\n",
      "\n",
      "Email 2:\n",
      "Subject: Rules not working\n",
      "Body: We created a rule to auto-assign emails based on subject line but it stopped working since yesterday...\n",
      "\n",
      "Email 3:\n",
      "Subject: Email stuck in pending\n",
      "Body: One of our emails is stuck in pending even after marking it resolved. Not sure what's happening....\n",
      "\n",
      "Email 4:\n",
      "Subject: Automation creating duplicate tasks\n",
      "Body: Your automation engine is creating 2 tasks for every email. This started after we edited our workflo...\n",
      "\n",
      "Email 5:\n",
      "Subject: Tags missing\n",
      "Body: Many of our tags are not appearing for new emails. Looks like the tagging model is not working for u...\n",
      "\n",
      "Email 6:\n",
      "Subject: Billing query\n",
      "Body: We were charged incorrectly this month. Need a corrected invoice....\n",
      "\n",
      "Email 7:\n",
      "Subject: CSAT not visible\n",
      "Body: CSAT scores disappeared from our dashboard today. Is there an outage?...\n",
      "\n",
      "Email 8:\n",
      "Subject: Delay in email loading\n",
      "Body: Opening a conversation takes 8–10 seconds. This is affecting our productivity....\n",
      "\n",
      "Email 9:\n",
      "Subject: Need help setting up SLAs\n",
      "Body: We want to configure SLAs for different customer tiers. Can someone guide us?...\n",
      "\n",
      "Email 10:\n",
      "Subject: Mail merge failing\n",
      "Body: Mail merge is not sending emails even though the CSV is correct....\n"
     ]
    }
   ],
   "source": [
    "# Load small dataset and select 10 emails\n",
    "df = pd.read_csv('../data/small_dataset.csv')\n",
    "\n",
    "# Select 10 emails for testing\n",
    "test_emails = df.head(10).copy()\n",
    "\n",
    "print(f\"Test set: {len(test_emails)} emails\")\n",
    "print(\"\\nSample emails:\")\n",
    "for idx, row in test_emails.iterrows():\n",
    "    print(f\"\\nEmail {row['email_id']}:\")\n",
    "    print(f\"Subject: {row['subject']}\")\n",
    "    print(f\"Body: {row['body'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt v1: Initial Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt v1:\n",
      "Analyze the sentiment of this customer support email.\n",
      "\n",
      "Subject: {subject}\n",
      "Body: {body}\n",
      "\n",
      "Classify the sentiment as: positive, negative, or neutral.\n",
      "\n",
      "Provide your response in JSON format:\n",
      "{{\n",
      "    \"sentiment\": \"positive/negative/neutral\",\n",
      "    \"confidence\": 0.0-1.0,\n",
      "    \"reasoning\": \"brief explanation of why you chose this sentiment\"\n",
      "}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT_V1 = \"\"\"Analyze the sentiment of this customer support email.\n",
    "\n",
    "Subject: {subject}\n",
    "Body: {body}\n",
    "\n",
    "Classify the sentiment as: positive, negative, or neutral.\n",
    "\n",
    "Provide your response in JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"reasoning\": \"brief explanation of why you chose this sentiment\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Save prompt v1\n",
    "with open('prompt_v1.txt', 'w') as f:\n",
    "    f.write(PROMPT_V1)\n",
    "\n",
    "print(\"Prompt v1:\")\n",
    "print(PROMPT_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Prompt v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 1: neutral (confidence: 0.8)\n",
      "Email 2: negative (confidence: 0.8)\n",
      "Email 3: neutral (confidence: 0.8)\n",
      "Email 4: negative (confidence: 0.8)\n",
      "Email 5: negative (confidence: 0.8)\n",
      "Email 6: negative (confidence: 0.8)\n",
      "Email 7: neutral (confidence: 0.8)\n",
      "Email 8: negative (confidence: 0.9)\n",
      "Email 9: neutral (confidence: 0.8)\n",
      "Email 10: negative (confidence: 0.8)\n"
     ]
    }
   ],
   "source": [
    "def analyze_sentiment(subject, body, prompt_template, client):\n",
    "    \"\"\"\n",
    "    Analyze sentiment using given prompt template.\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(subject=subject, body=body)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        # Get response text\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Strip markdown code fences if present\n",
    "        if '```json' in response_text or '```' in response_text:\n",
    "            # Find the JSON block\n",
    "            if '```json' in response_text:\n",
    "                # Extract content between ```json and ```\n",
    "                start = response_text.find('```json') + 7\n",
    "                end = response_text.find('```', start)\n",
    "                response_text = response_text[start:end].strip()\n",
    "            elif response_text.strip().startswith('```'):\n",
    "                # Remove ```json or ``` at start and ``` at end\n",
    "                response_text = response_text.strip()\n",
    "                response_text = response_text.split('\\n', 1)[1]  # Remove first line (```)\n",
    "                response_text = response_text.rsplit('\\n', 1)[0]  # Remove last line (```)\n",
    "        \n",
    "        result = json.loads(response_text)\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"sentiment\": \"error\", \"confidence\": 0.0, \"reasoning\": str(e)}\n",
    "\n",
    "# Test on all 10 emails\n",
    "results_v1 = []\n",
    "\n",
    "for idx, row in test_emails.iterrows():\n",
    "    result = analyze_sentiment(row['subject'], row['body'], PROMPT_V1, client)\n",
    "    results_v1.append({\n",
    "        'email_id': row['email_id'],\n",
    "        'subject': row['subject'],\n",
    "        'sentiment': result['sentiment'],\n",
    "        'confidence': result['confidence'],\n",
    "        'reasoning': result['reasoning']\n",
    "    })\n",
    "    print(f\"Email {row['email_id']}: {result['sentiment']} (confidence: {result['confidence']})\")\n",
    "\n",
    "# Save results\n",
    "results_v1_df = pd.DataFrame(results_v1)\n",
    "results_v1_df.to_json('results_v1.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manual Evaluation of v1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt v1 Results:\n",
      "================================================================================\n",
      "\n",
      "Email 1:\n",
      "Subject: Unable to access shared mailbox\n",
      "Sentiment: neutral\n",
      "Confidence: 0.8\n",
      "Reasoning: The customer is reporting an issue, but the tone is polite and matter-of-fact, without expressing frustration, anger, or dissatisfaction, which are typical characteristics of negative sentiment. The language used is also neutral, focusing on the problem and the request for assistance, rather than making a positive or negative comment.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email 2:\n",
      "Subject: Rules not working\n",
      "Sentiment: negative\n",
      "Confidence: 0.8\n",
      "Reasoning: The customer is reporting an issue with a feature (rules not working) that is causing inconvenience, indicating a negative experience with the product.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email 3:\n",
      "Subject: Email stuck in pending\n",
      "Sentiment: neutral\n",
      "Confidence: 0.8\n",
      "Reasoning: The customer is reporting an issue, but their tone is matter-of-fact and inquiring, without expressing frustration, anger, or dissatisfaction, which are typical characteristics of negative sentiment. The language used is also neutral, without any positive or enthusiastic tone, which is why it's not classified as positive.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email 4:\n",
      "Subject: Automation creating duplicate tasks\n",
      "Sentiment: negative\n",
      "Confidence: 0.8\n",
      "Reasoning: The customer is reporting a problem with the automation engine, specifically that it is creating duplicate tasks, which indicates a issue or frustration with the product.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email 5:\n",
      "Subject: Tags missing\n",
      "Sentiment: negative\n",
      "Confidence: 0.8\n",
      "Reasoning: The customer is reporting an issue with the tagging model not working, which indicates a problem with the product or service. The tone is direct and objective, but the content of the message expresses dissatisfaction, leading to a negative sentiment classification.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email 6:\n",
      "Subject: Billing query\n",
      "Sentiment: negative\n",
      "Confidence: 0.8\n",
      "Reasoning: The customer states they were 'charged incorrectly', which indicates a problem with their billing, and requests a 'corrected invoice', implying dissatisfaction with the current state of their account.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email 7:\n",
      "Subject: CSAT not visible\n",
      "Sentiment: neutral\n",
      "Confidence: 0.8\n",
      "Reasoning: The customer is reporting an issue with their dashboard, but the tone is informative and inquiring, without expressing frustration, anger, or dissatisfaction, which are typical characteristics of negative sentiment. The language used is straightforward and neutral, aiming to seek assistance rather than complain.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email 8:\n",
      "Subject: Delay in email loading\n",
      "Sentiment: negative\n",
      "Confidence: 0.9\n",
      "Reasoning: The customer is reporting a delay in email loading, which is affecting their productivity, indicating a problem that is causing frustration or inconvenience.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email 9:\n",
      "Subject: Need help setting up SLAs\n",
      "Sentiment: neutral\n",
      "Confidence: 0.8\n",
      "Reasoning: The customer is seeking help and guidance, which is a neutral request. There is no emotional language or tone that suggests a positive or negative sentiment, such as frustration, appreciation, or excitement.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Email 10:\n",
      "Subject: Mail merge failing\n",
      "Sentiment: negative\n",
      "Confidence: 0.8\n",
      "Reasoning: The customer is reporting a problem with the mail merge feature, stating that it's not working as expected, which indicates a negative experience with the product.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display results for manual review\n",
    "print(\"\\nPrompt v1 Results:\")\n",
    "print(\"=\"*80)\n",
    "for idx, result in enumerate(results_v1, 1):\n",
    "    print(f\"\\nEmail {idx}:\")\n",
    "    print(f\"Subject: {result['subject']}\")\n",
    "    print(f\"Sentiment: {result['sentiment']}\")\n",
    "    print(f\"Confidence: {result['confidence']}\")\n",
    "    print(f\"Reasoning: {result['reasoning']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# TODO: Manually assess each prediction\n",
    "# - Is the sentiment correct?\n",
    "# - Is the confidence appropriate?\n",
    "# - Is the reasoning sound?\n",
    "# - What patterns of errors do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Failures and Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Issues to address in v2:\n",
      "1. [Issue 1]\n",
      "2. [Issue 2]\n",
      "3. [Issue 3]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Document issues found in v1\n",
    "# Examples of things to look for:\n",
    "# - Incorrect sentiment classifications\n",
    "# - Overconfident or underconfident predictions\n",
    "# - Poor reasoning\n",
    "# - Inconsistent handling of similar emails\n",
    "# - Edge cases not handled well\n",
    "\n",
    "print(\"\\nIssues to address in v2:\")\n",
    "print(\"1. [Issue 1]\")\n",
    "print(\"2. [Issue 2]\")\n",
    "print(\"3. [Issue 3]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prompt v2: Improved Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt v2 created with improvements.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Design improved prompt based on v1 failures\n",
    "# Consider adding:\n",
    "# - More specific instructions\n",
    "# - Examples (few-shot learning)\n",
    "# - Clearer definitions of positive/negative/neutral\n",
    "# - Guidelines for confidence scoring\n",
    "# - Context about customer support domain\n",
    "\n",
    "PROMPT_V2 = \"\"\"You are an expert at analyzing sentiment in customer support emails.\n",
    "\n",
    "Email to analyze:\n",
    "Subject: {subject}\n",
    "Body: {body}\n",
    "\n",
    "Instructions:\n",
    "1. Classify the sentiment as:\n",
    "   - \"positive\": Customer is happy, grateful, or satisfied\n",
    "   - \"negative\": Customer is frustrated, angry, or disappointed\n",
    "   - \"neutral\": Informational query or neither clearly positive nor negative\n",
    "\n",
    "2. Consider:\n",
    "   - Tone and word choice\n",
    "   - Urgency markers\n",
    "   - Emotional indicators\n",
    "   - Context of the issue\n",
    "\n",
    "3. Confidence scoring:\n",
    "   - High (0.8-1.0): Clear sentiment indicators\n",
    "   - Medium (0.5-0.79): Some ambiguity\n",
    "   - Low (0.0-0.49): Mixed signals or unclear\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"reasoning\": \"detailed explanation referencing specific words/phrases\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Save prompt v2\n",
    "with open('prompt_v2.txt', 'w') as f:\n",
    "    f.write(PROMPT_V2)\n",
    "\n",
    "print(\"Prompt v2 created with improvements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Prompt v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 1: neutral (confidence: 0.8)\n",
      "Email 2: negative (confidence: 0.8)\n",
      "Email 3: neutral (confidence: 0.7)\n",
      "Email 4: negative (confidence: 0.8)\n",
      "Email 5: negative (confidence: 0.8)\n",
      "Email 6: negative (confidence: 0.8)\n",
      "Email 7: neutral (confidence: 0.8)\n",
      "Email 8: negative (confidence: 0.9)\n",
      "Email 9: neutral (confidence: 0.9)\n",
      "Email 10: negative (confidence: 0.8)\n"
     ]
    }
   ],
   "source": [
    "# Test v2 on same 10 emails\n",
    "results_v2 = []\n",
    "\n",
    "for idx, row in test_emails.iterrows():\n",
    "    result = analyze_sentiment(row['subject'], row['body'], PROMPT_V2, client)\n",
    "    results_v2.append({\n",
    "        'email_id': row['email_id'],\n",
    "        'subject': row['subject'],\n",
    "        'sentiment': result['sentiment'],\n",
    "        'confidence': result['confidence'],\n",
    "        'reasoning': result['reasoning']\n",
    "    })\n",
    "    print(f\"Email {row['email_id']}: {result['sentiment']} (confidence: {result['confidence']})\")\n",
    "\n",
    "# Save results\n",
    "results_v2_df = pd.DataFrame(results_v2)\n",
    "results_v2_df.to_json('results_v2.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare v1 vs v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of v1 vs v2:\n",
      "   email_id                              subject v1_sentiment  v1_confidence  \\\n",
      "0         1      Unable to access shared mailbox      neutral            0.8   \n",
      "1         2                    Rules not working     negative            0.8   \n",
      "2         3               Email stuck in pending      neutral            0.8   \n",
      "3         4  Automation creating duplicate tasks     negative            0.8   \n",
      "4         5                         Tags missing     negative            0.8   \n",
      "5         6                        Billing query     negative            0.8   \n",
      "6         7                     CSAT not visible      neutral            0.8   \n",
      "7         8               Delay in email loading     negative            0.9   \n",
      "8         9            Need help setting up SLAs      neutral            0.8   \n",
      "9        10                   Mail merge failing     negative            0.8   \n",
      "\n",
      "  v2_sentiment  v2_confidence  changed  \n",
      "0      neutral            0.8    False  \n",
      "1     negative            0.8    False  \n",
      "2      neutral            0.7    False  \n",
      "3     negative            0.8    False  \n",
      "4     negative            0.8    False  \n",
      "5     negative            0.8    False  \n",
      "6      neutral            0.8    False  \n",
      "7     negative            0.9    False  \n",
      "8      neutral            0.9    False  \n",
      "9     negative            0.8    False  \n",
      "\n",
      "Number of changes: 0\n",
      "\n",
      "Average confidence v1: 0.810\n",
      "Average confidence v2: 0.810\n"
     ]
    }
   ],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'email_id': results_v1_df['email_id'],\n",
    "    'subject': results_v1_df['subject'],\n",
    "    'v1_sentiment': results_v1_df['sentiment'],\n",
    "    'v1_confidence': results_v1_df['confidence'],\n",
    "    'v2_sentiment': results_v2_df['sentiment'],\n",
    "    'v2_confidence': results_v2_df['confidence']\n",
    "})\n",
    "\n",
    "# Identify changes\n",
    "comparison['changed'] = comparison['v1_sentiment'] != comparison['v2_sentiment']\n",
    "\n",
    "print(\"\\nComparison of v1 vs v2:\")\n",
    "print(comparison)\n",
    "\n",
    "print(f\"\\nNumber of changes: {comparison['changed'].sum()}\")\n",
    "print(f\"\\nAverage confidence v1: {comparison['v1_confidence'].mean():.3f}\")\n",
    "print(f\"Average confidence v2: {comparison['v2_confidence'].mean():.3f}\")\n",
    "\n",
    "# Show cases where prediction changed\n",
    "if comparison['changed'].any():\n",
    "    print(\"\\nEmails where prediction changed:\")\n",
    "    print(comparison[comparison['changed']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Document Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next steps:\n",
      "1. Manually review all results\n",
      "2. Calculate accuracy (need ground truth labels)\n",
      "3. Update evaluation_report.md with findings\n",
      "4. Document systematic evaluation process\n"
     ]
    }
   ],
   "source": [
    "# TODO: Document in evaluation_report.md:\n",
    "# 1. What failed in v1\n",
    "# 2. What was improved in v2\n",
    "# 3. How to evaluate prompts systematically\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Manually review all results\")\n",
    "print(\"2. Calculate accuracy (need ground truth labels)\")\n",
    "print(\"3. Update evaluation_report.md with findings\")\n",
    "print(\"4. Document systematic evaluation process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Distribution:\n",
      "\n",
      "v1:\n",
      "sentiment\n",
      "negative    6\n",
      "neutral     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "v2:\n",
      "sentiment\n",
      "negative    6\n",
      "neutral     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confidence Stats:\n",
      "\n",
      "v1:\n",
      "count    10.000000\n",
      "mean      0.810000\n",
      "std       0.031623\n",
      "min       0.800000\n",
      "25%       0.800000\n",
      "50%       0.800000\n",
      "75%       0.800000\n",
      "max       0.900000\n",
      "Name: confidence, dtype: float64\n",
      "\n",
      "v2:\n",
      "count    10.000000\n",
      "mean      0.810000\n",
      "std       0.056765\n",
      "min       0.700000\n",
      "25%       0.800000\n",
      "50%       0.800000\n",
      "75%       0.800000\n",
      "max       0.900000\n",
      "Name: confidence, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Sentiment distribution\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(\"\\nv1:\")\n",
    "print(results_v1_df['sentiment'].value_counts())\n",
    "print(\"\\nv2:\")\n",
    "print(results_v2_df['sentiment'].value_counts())\n",
    "\n",
    "# Confidence distribution\n",
    "print(\"\\nConfidence Stats:\")\n",
    "print(\"\\nv1:\")\n",
    "print(results_v1_df['confidence'].describe())\n",
    "print(\"\\nv2:\")\n",
    "print(results_v2_df['confidence'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
