{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Sentiment Analysis Prompt Evaluation\n",
    "\n",
    "## Objective\n",
    "- Create sentiment analysis prompt (positive/negative/neutral)\n",
    "- Include confidence score and reasoning\n",
    "- Test on 10 emails\n",
    "- Iterate and improve (v1 â†’ v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data (10 Emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small dataset and select 10 emails\n",
    "df = pd.read_csv('../data/small_dataset.csv')\n",
    "\n",
    "# Select 10 emails for testing\n",
    "test_emails = df.head(10).copy()\n",
    "\n",
    "print(f\"Test set: {len(test_emails)} emails\")\n",
    "print(\"\\nSample emails:\")\n",
    "for idx, row in test_emails.iterrows():\n",
    "    print(f\"\\nEmail {row['email_id']}:\")\n",
    "    print(f\"Subject: {row['subject']}\")\n",
    "    print(f\"Body: {row['body'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt v1: Initial Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V1 = \"\"\"Analyze the sentiment of this customer support email.\n",
    "\n",
    "Subject: {subject}\n",
    "Body: {body}\n",
    "\n",
    "Classify the sentiment as: positive, negative, or neutral.\n",
    "\n",
    "Provide your response in JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"reasoning\": \"brief explanation of why you chose this sentiment\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Save prompt v1\n",
    "with open('prompt_v1.txt', 'w') as f:\n",
    "    f.write(PROMPT_V1)\n",
    "\n",
    "print(\"Prompt v1:\")\n",
    "print(PROMPT_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Prompt v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_sentiment(subject, body, prompt_template, client):\n    \"\"\"\n    Analyze sentiment using given prompt template.\n    \"\"\"\n    prompt = prompt_template.format(subject=subject, body=body)\n    \n    try:\n        response = client.chat.completions.create(\n            model=\"llama-3.3-70b-versatile\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.1,\n            max_tokens=200\n        )\n        \n        result = json.loads(response.choices[0].message.content)\n        return result\n    \n    except Exception as e:\n        return {\"sentiment\": \"error\", \"confidence\": 0.0, \"reasoning\": str(e)}\n\n# Test on all 10 emails\nresults_v1 = []\n\nfor idx, row in test_emails.iterrows():\n    result = analyze_sentiment(row['subject'], row['body'], PROMPT_V1, client)\n    results_v1.append({\n        'email_id': row['email_id'],\n        'subject': row['subject'],\n        'sentiment': result['sentiment'],\n        'confidence': result['confidence'],\n        'reasoning': result['reasoning']\n    })\n    print(f\"Email {row['email_id']}: {result['sentiment']} (confidence: {result['confidence']})\")\n\n# Save results\nresults_v1_df = pd.DataFrame(results_v1)\nresults_v1_df.to_json('results_v1.json', orient='records', indent=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manual Evaluation of v1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for manual review\n",
    "print(\"\\nPrompt v1 Results:\")\n",
    "print(\"=\"*80)\n",
    "for idx, result in enumerate(results_v1, 1):\n",
    "    print(f\"\\nEmail {idx}:\")\n",
    "    print(f\"Subject: {result['subject']}\")\n",
    "    print(f\"Sentiment: {result['sentiment']}\")\n",
    "    print(f\"Confidence: {result['confidence']}\")\n",
    "    print(f\"Reasoning: {result['reasoning']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# TODO: Manually assess each prediction\n",
    "# - Is the sentiment correct?\n",
    "# - Is the confidence appropriate?\n",
    "# - Is the reasoning sound?\n",
    "# - What patterns of errors do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Failures and Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Document issues found in v1\n",
    "# Examples of things to look for:\n",
    "# - Incorrect sentiment classifications\n",
    "# - Overconfident or underconfident predictions\n",
    "# - Poor reasoning\n",
    "# - Inconsistent handling of similar emails\n",
    "# - Edge cases not handled well\n",
    "\n",
    "print(\"\\nIssues to address in v2:\")\n",
    "print(\"1. [Issue 1]\")\n",
    "print(\"2. [Issue 2]\")\n",
    "print(\"3. [Issue 3]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prompt v2: Improved Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design improved prompt based on v1 failures\n",
    "# Consider adding:\n",
    "# - More specific instructions\n",
    "# - Examples (few-shot learning)\n",
    "# - Clearer definitions of positive/negative/neutral\n",
    "# - Guidelines for confidence scoring\n",
    "# - Context about customer support domain\n",
    "\n",
    "PROMPT_V2 = \"\"\"You are an expert at analyzing sentiment in customer support emails.\n",
    "\n",
    "Email to analyze:\n",
    "Subject: {subject}\n",
    "Body: {body}\n",
    "\n",
    "Instructions:\n",
    "1. Classify the sentiment as:\n",
    "   - \"positive\": Customer is happy, grateful, or satisfied\n",
    "   - \"negative\": Customer is frustrated, angry, or disappointed\n",
    "   - \"neutral\": Informational query or neither clearly positive nor negative\n",
    "\n",
    "2. Consider:\n",
    "   - Tone and word choice\n",
    "   - Urgency markers\n",
    "   - Emotional indicators\n",
    "   - Context of the issue\n",
    "\n",
    "3. Confidence scoring:\n",
    "   - High (0.8-1.0): Clear sentiment indicators\n",
    "   - Medium (0.5-0.79): Some ambiguity\n",
    "   - Low (0.0-0.49): Mixed signals or unclear\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"reasoning\": \"detailed explanation referencing specific words/phrases\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Save prompt v2\n",
    "with open('prompt_v2.txt', 'w') as f:\n",
    "    f.write(PROMPT_V2)\n",
    "\n",
    "print(\"Prompt v2 created with improvements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Prompt v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v2 on same 10 emails\n",
    "results_v2 = []\n",
    "\n",
    "for idx, row in test_emails.iterrows():\n",
    "    result = analyze_sentiment(row['subject'], row['body'], PROMPT_V2, client)\n",
    "    results_v2.append({\n",
    "        'email_id': row['email_id'],\n",
    "        'subject': row['subject'],\n",
    "        'sentiment': result['sentiment'],\n",
    "        'confidence': result['confidence'],\n",
    "        'reasoning': result['reasoning']\n",
    "    })\n",
    "    print(f\"Email {row['email_id']}: {result['sentiment']} (confidence: {result['confidence']})\")\n",
    "\n",
    "# Save results\n",
    "results_v2_df = pd.DataFrame(results_v2)\n",
    "results_v2_df.to_json('results_v2.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare v1 vs v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'email_id': results_v1_df['email_id'],\n",
    "    'subject': results_v1_df['subject'],\n",
    "    'v1_sentiment': results_v1_df['sentiment'],\n",
    "    'v1_confidence': results_v1_df['confidence'],\n",
    "    'v2_sentiment': results_v2_df['sentiment'],\n",
    "    'v2_confidence': results_v2_df['confidence']\n",
    "})\n",
    "\n",
    "# Identify changes\n",
    "comparison['changed'] = comparison['v1_sentiment'] != comparison['v2_sentiment']\n",
    "\n",
    "print(\"\\nComparison of v1 vs v2:\")\n",
    "print(comparison)\n",
    "\n",
    "print(f\"\\nNumber of changes: {comparison['changed'].sum()}\")\n",
    "print(f\"\\nAverage confidence v1: {comparison['v1_confidence'].mean():.3f}\")\n",
    "print(f\"Average confidence v2: {comparison['v2_confidence'].mean():.3f}\")\n",
    "\n",
    "# Show cases where prediction changed\n",
    "if comparison['changed'].any():\n",
    "    print(\"\\nEmails where prediction changed:\")\n",
    "    print(comparison[comparison['changed']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Document Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Document in evaluation_report.md:\n",
    "# 1. What failed in v1\n",
    "# 2. What was improved in v2\n",
    "# 3. How to evaluate prompts systematically\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Manually review all results\")\n",
    "print(\"2. Calculate accuracy (need ground truth labels)\")\n",
    "print(\"3. Update evaluation_report.md with findings\")\n",
    "print(\"4. Document systematic evaluation process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(\"\\nv1:\")\n",
    "print(results_v1_df['sentiment'].value_counts())\n",
    "print(\"\\nv2:\")\n",
    "print(results_v2_df['sentiment'].value_counts())\n",
    "\n",
    "# Confidence distribution\n",
    "print(\"\\nConfidence Stats:\")\n",
    "print(\"\\nv1:\")\n",
    "print(results_v1_df['confidence'].describe())\n",
    "print(\"\\nv2:\")\n",
    "print(results_v2_df['confidence'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}